<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TensorTales</title>
    <link>https://markkurzeja.github.io/</link>
    <description>Recent content on TensorTales</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 23 Mar 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://markkurzeja.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Are Token-wise MLPs over-parameterized in Transformers?</title>
      <link>https://markkurzeja.github.io/post/rff/</link>
      <pubDate>Sat, 23 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://markkurzeja.github.io/post/rff/</guid>
      <description>&amp;ldquo;The pessimist says the glass is half-full. The optimist says the glass is half-empty. The engineer says its mearly 2x larger than it needs to be. \( \approx \) Thomas Cathcart&#xA;To spoil the punchline, there is a curious connection between the token-wise MLP projections of modern Transformer recipes and Random Fourier Features.&#xA;Some adhoc experiments show a tiny tweak to MLPs allow them to train with far smaller batch sizes and dramatically reduce the number of parameters required to learn complex functions.</description>
    </item>
    <item>
      <title>Random Fourier Features, Part I </title>
      <link>https://markkurzeja.github.io/post/rff_intro/</link>
      <pubDate>Sat, 23 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://markkurzeja.github.io/post/rff_intro/</guid>
      <description>Random Fourier Features, in my opinion, have one of the best performance-to-cost tradeoffs in machine learning techniques today. Simple to code, cheap to fit, and unreasonably effective, they have been my bread-and-butter for small-to-medium multi-dimensional learning tasks and serve as an amazing baseline for more complex systems.&#xA;This post will be the first of a three part series. To give you a roadmap:&#xA;Post I: This post will help to build intuition of RFF through low-dimensional examples and explain some of the code to come.</description>
    </item>
    <item>
      <title>TensorTales</title>
      <link>https://markkurzeja.github.io/post/tensor-tales/</link>
      <pubDate>Sun, 17 Mar 2024 14:48:38 -0500</pubDate>
      <guid>https://markkurzeja.github.io/post/tensor-tales/</guid>
      <description>I&amp;rsquo;ve always loved machine learning. Something about it has always felt like magic. When I first learned about prediction machines, in high school, to me, they always conjured images of The Oracle of Delphi or one of those old cartoons with a group of magicians huddled around a crystal ball. Ask the right questions, cast the right spells in the right order, and the seemingly impossible, a glimpse of the future, could be yours.</description>
    </item>
    <item>
      <title>About me</title>
      <link>https://markkurzeja.github.io/page/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://markkurzeja.github.io/page/about/</guid>
      <description>I am a Senior Software Engineer at Google. My current focus is on using machine learning to combat malware on Android devices. I have my BBA from the Ross School of Business at the University of Michigan and a Masters in Statistics from the Racknam Graduate School at the University of Michigan. My research interests are in machine learning generally, including deep learning, business, and statistics.</description>
    </item>
  </channel>
</rss>
